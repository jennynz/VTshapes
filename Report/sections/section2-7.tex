\section{Methodology validation \& critique}\label{sec:methodology-validation}

\subsection{Validation of image processing process}

This procedure of extracting area functions from MRI images was validated across several stages. First, careful note was taken from the reports produced by previous researchers with the aim of minimising error and noise introduced in the manual step of marking up the vocal tract boundaries. Most of the vocal tracts had already been marked up by previous researchers, and their derived area functions would make up the majority of my data set once I added in the final two speakers' area functions (VT04 and VT07). I ensured that it would be valid to compare and combine these last area functions by closely following the suggestions given in previous reports and theses \cite{helen, daniel, kalyan}, inspecting their figures and displaying their boundaries on CMGUI using \verb|data_point_viewer.com|. Secondly, during the correlation analyses of the area functions between speakers, it was noted that the area functions were strongly correlated between speakers, despite the data set being a combination of area functions from three different users at different times (Figure \ref{fig:inter_PC1}).

The variability of this manual mark-up procedure was quantified by repeating the process for a few of the vowels which had already been marked up by myself, to see the variations or correlations which occur between then same speaker, same vowel and same user doing the markup. I repeated the four cardinal vowels of VT04 (/\textipa{i:}, \textipa{\ae}, \textipa{6}, \textipa{u}/, Set 1), and performed a Pearson product-moment correlation between the first three principal components of the area functions from the two repeated experiments. Given that these are area functions derived from the same datasets, any deviation from 100\% correlation was a measure of the variability inherent in this extraction procedure. [RESULTS HERE]

\subsection{Limitations of CMGUI}

There are several quirks to the CMGUI Scene Editor and Graphics Window which must be noted when attempting this step. These are outlined in Appendix \ref{ch:cmgui}. Also, many of the interpolated slices showed rather unclear, blurry and often asymmetrical structures. Bony structures such as teeth also show up with low signal due to their low water content, often intersecting with the dark areas or air indicating the vocal tract. Physiological assumptions were therefore often used to deduce the outlines of the vocal tract, such as assuming symmetry and fairly regular, elliptic shapes where appropriate (e.g. in the pharyngeal cavity).

It is for these reasons that an automated method of identifying the vocal tract in these sections would be difficult to achieve, despite the fact that automation would improve repeatability and eliminate the only remaining manual (and very time-consuming) process in the pipeline. Increasing the number of sagittal slices taken in the raw data set may help with these ambiguities; however, MRI data acquisition is a slow and rather strenuous process particularly for the participants, who must hold completely still while articulating these vowel sounds for long periods of time. [Given the previously mentioned quantification of variability between users who mark up the vocal track boundaries, we can conclude that the benefits of being able to judge the structures in each plane on a case by case basis outweigh the human errors introduced.]

Other limitations to the CMGUI step of the image processing pipeline was the potential error introduced by the large step sizes during the cross-sectioning of the vocal tract. [Explain with figures]

\subsection{Improvements made to image processing pipeline}

At the beginning of this project, I was given a set of resources from previous research periods including raw MRI data, MATLAB and R scripts, and reports from previous research students \cite{helen, daniel, kalyan}. The data of the 12 speakers was split among the different periods, with five of the speakers having had both sets of data analysed by Daniel, and the rest of the speakers bar VT04 and VT07 having one set per vowel analysed by Helen. All of this data was consolidated into one directory which contained a consistent file structure for clarity, and to allow automatic reading of files in my R scripts.

Processing the raw MRI images for VT04 and VT07 involved several tedious steps, such as copying and pasting multiple files across several directories and renaming certain files and lines of code to match the vowel or speaker at hand. To speed up this process, I wrote a batch file which could be pasted into a Windows Command Prompt (\verb|cmd.exe|), punctuated by steps where tasks had to be carried out on CMGUI. This sped up the process considerably, leaving only the absolutely necessary steps for manual processing.

Several text files containing R commands and console outputs were provided by Dr Catherine Watson. The existing code was reworked to accommodate a variable data set with any number of speakers, sets and vowels. The functions were also simplified in their number of inputs, and generalised to improve versatility and re-usability.